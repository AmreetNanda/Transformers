{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696b5e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c623aa0",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "Transformer library features, the library downloads the pre-trained models for natural language understanding (NLU Tasks) such as analyzing the sentiments of a text and NLG (Natural Language Generation), such as completing a prompt with new text or translating in another language.\n",
    "\n",
    "First we will see how to easily leverage hte pipelines API to quickly use those pre-trained models at interface. Thus we will dig a little bitmore and see how the library gives access to those models and helps in preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ea094",
   "metadata": {},
   "source": [
    "##### USE_CASE\n",
    "1. Sentiment Analysis : Whether positive or negetive\n",
    "2. Text Generation : Provide a prompt and model will generate what follows\n",
    "3. Name Entity Recognition (NER) : In an input sentence, label each word with entity it represents (person, place)\n",
    "4. Question Answering : Provide the model with same context and a question, extract the answers from the context\n",
    "5. Filling masked text : Given a text with masked word and fill in the blanks\n",
    "6. Summerization : Generate a summary of long text\n",
    "7. Translation : Translates a text into another language\n",
    "8. Feature Extraction : Return a tensor representation of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9758c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amreet\\Desktop\\Transformers\\transformer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Amreet\\Desktop\\Transformers\\transformer\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Amreet\\Desktop\\Transformers\\transformer\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amreet\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "# Note: Transformer requires pytorch to be installed else will trow error \"NameError: torch is not present\"\n",
    "# Note: Transformer does not support keras 3 yet so have to install tf-keras in the environment else will throw NameError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab841e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998239874839783}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('We are very happy to show you the transformer library')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977c7bf",
   "metadata": {},
   "source": [
    "when the above command is used, a pre-trained model and its tokenizer is downloaded and cached. As an introduction, the tokenizer's job is to preprocess the text for the model, which is then responsible for making predictions. The pipeline groups all of that together and post-process the predictions to make them readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710b8ee",
   "metadata": {},
   "source": [
    "By default the model downloaded for the pipeline is \"distillbert-bert-uncased-finetuned-sst-2-english\". It uses the Distillbert architecture and has been finetuned on a dataset called SST-2 for the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdac269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:POSITIVE, with score of 0.9998\n",
      "label:POSITIVE, with score of 0.9998\n",
      "label:POSITIVE, with score of 0.9994\n",
      "label:NEGATIVE, with score of 0.9883\n",
      "label:POSITIVE, with score of 0.9993\n"
     ]
    }
   ],
   "source": [
    "results = classifier(\n",
    "    [\n",
    "        \"We hope you like the food\",\n",
    "        \"We are very happy to show you the transformer library\",\n",
    "        \"Taste is not very good but is manageable\",\n",
    "        \"I don't like to work hard\",\n",
    "        \"I prefer to work smart\"\n",
    "    ]\n",
    ")\n",
    "for result in results:\n",
    "    print(f\"label:{result['label']}, with score of {round(result['score'],4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ab2607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9682278633117676}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"esperamos que no lo odie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d64b2b",
   "metadata": {},
   "source": [
    "### model used for above :  [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "Note: These models are present in Huggingface "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493ceb2",
   "metadata": {},
   "source": [
    "Now lets say that we want to use another model, for example that is trained on German data, we can search through the models in HuggingFace that gathers most pre-trained models done by research labs. \n",
    "For different language the model that can be used is [nlptown/bert-base-multilingual-uncased-sentiment](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) \n",
    "\n",
    "For that we need 2 classes \n",
    "1. AutoTokenizer\n",
    "2. AutoModelForSequenceClassification or TFAutoModelForSequenceClassification\n",
    "\n",
    "AutoTokenizer --> Takes the text data and convert it to some numerical data just like word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088eb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf16135",
   "metadata": {},
   "source": [
    "##### Fine_Tune\n",
    "Now if we don't find a model that has been pre-trained on same similar data as ours, then we need to fine-tune a pre-trained model on our data\n",
    "\n",
    "Under the hood of pre-trained model\n",
    "- First the Tokenizer is responsible for preprocessing the text. It will split the given text into words (or part of words, punctuation and symbols) usually called Tokens\n",
    "- There are multiple rules that governs the process which is why we need to initialize the tokenizer using the name of the model ensuring that we are using the same rules using which the model was trained\n",
    "- The next step is to convert these tokens into numbers and to be able to build the tensors out of them and feed to the model\n",
    "- To do this the Tokenizer has a 'vocab' which is the part we download when we instantiate it with the 'from_pretrained' method, since we have to use the same vocab as when the model was pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43cd1ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amreet\\Desktop\\Transformers\\transformer\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amreet\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Amreet\\Desktop\\Transformers\\transformer\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a392e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998760223388672}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am a good developer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d54cfb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 10938, 2121, 3075, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the transformer library\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a7d50",
   "metadata": {},
   "source": [
    "The above line returns a dictionary string to list of ints containing the ids of the token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25ca7b",
   "metadata": {},
   "source": [
    "modifying the tokenizer for length and create batch for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f3fd52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 10938, 2121, 3075, 102]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "tf_batch = tokenizer(\n",
    "    [\"We are very happy to show you the transformer library\"],\n",
    "    padding= True,\n",
    "    truncation = True,\n",
    "    max_length = 512,\n",
    "    return_tensors = 'tf'\n",
    ")\n",
    "\n",
    "for key, value in tf_batch.items():\n",
    "    print(f\"{key} : {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a47ad0",
   "metadata": {},
   "source": [
    "### Other models used for multilingual :  [nlptown/bert-base-multilingual-uncased-sentiment](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)\n",
    "Note: These models are present in Huggingface "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
